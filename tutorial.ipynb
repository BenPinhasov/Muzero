{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # Install dependency # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# Python 3.8.3 to 3.9.0 to 3.10 ( 3.10 is experimental with ray)\n",
    "# # # unQuote to install dependency\n",
    "# !pip install matplotlib\n",
    "# !pip install numpy\n",
    "\n",
    "# # # unQuote to install dependency\n",
    "# # # for Nvidia gpu / cpu version  # I didn't try with ROCm for AMD gpu (pytorch.org) if you want to look at it\n",
    "# update nvidia driver\n",
    "# download and install cuda from : https://developer.nvidia.com/cuda-11-6-1-download-archive\n",
    "# install cudnn :  https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html\n",
    "# !pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "# # # unQuote to install dependency\n",
    "# # # for cpu only | the model is fairly small for cartpole for example, so it should be able to run well on cpu only\n",
    "# !pip install torch torchvision torchaudio\n",
    "\n",
    "# # # unQuote to install dependency\n",
    "# # # Gym env for game simulation\n",
    "# # install for windows Microsoft C++ Build Tools 14+ (require for Box2D lib):\n",
    "# # https://visualstudio.microsoft.com/downloads/\n",
    "# !pip install gym[all]\n",
    "# !pip install gym[atari]\n",
    "# !pip install gym[accept-rom-license]\n",
    "# # or using conda\n",
    "# conda install -c conda-forge gym-all\n",
    "# if you can't install Box2D, try :\n",
    "# for windows: conda install -c anaconda swig\n",
    "# or else try the dockerfile.\n",
    "\n",
    "# # # Ray install\n",
    "# pip install -U \"ray[default]\"\n",
    "\n",
    "# #### OR #####\n",
    "# # replace  \" !pip3 \"  by \" !pip \" depending on the OS\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "# # # To review specific Gym env observation and action space:\n",
    "# import gym\n",
    "# print(gym.envs.registry.items())\n",
    "# # # to check observation space and action space of a particular env\n",
    "# import gym\n",
    "# env_str = 'CarRacing-v0'\n",
    "# env = gym.make(env_str)\n",
    "# print(f'{env_str} :| observation space: {env.observation_space} | action space : {env.action_space} |')\n",
    "\n",
    "\n",
    "# # # To review observation space and action space of all env :\n",
    "# import gym\n",
    "# import warnings\n",
    "# for i in list(gym.envs.registry.items()):\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.simplefilter(\"ignore\")\n",
    "#         try:\n",
    "#             if 'gym.envs.mujoco' in i[1].__dict__[\"entry_point\"]:\n",
    "#                 pass \n",
    "#             else:\n",
    "#                 env = gym.make(i[0])\n",
    "#                 print(f'{i[0]} :| observation space: {type(env.action_space)} | action space : {type(env.observation_space)} |')\n",
    "#         except:pass\n",
    "\n",
    "\n",
    "# # # for cloud compute on multiple cluster: \n",
    "# https://docs.ray.io/en/latest/cluster/vms/getting-started.html#vm-cluster-quick-start\n",
    "# https://docs.ray.io/en/latest/train/train.html\n",
    "# add remote cluster address in ray.init() in self_play.py with remote ray kubernetes cluster\n",
    "# and wrap model muzero_model.train() into a ray.init()\n",
    "# and remote cluster address in ray.init() with remote ray kubernetes cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "from monte_carlo_tree_search import *\n",
    "from game import *\n",
    "from replay_buffer import *\n",
    "from self_play import *\n",
    "from muzero_model import *\n",
    "\n",
    "\n",
    "# # # unquote to print complet tensor\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # set game environment from gym library\n",
    "# # # render_mode should be set to None if you don't want rgb observation\n",
    "env = gym.make(\"CartPole-v1\", render_mode=None) \n",
    "# # # if you want want rgb observation set render_mode to \"rgb_array\" , \"human\" or the render_mode rgb of our env\n",
    "# # # so for example if you want to use vision_model, you will have to change it\n",
    "# env = gym.make(\"ALE/Asterix-v5\",render_mode='human')\n",
    "\n",
    "# # # the random seed are set to 0 for reproducibility purpose\n",
    "# # #  https://pytorch.org/docs/stable/notes/randomness.html\n",
    "seed = 0\n",
    "np.random.seed(seed) # set the random seed of numpy\n",
    "torch.manual_seed(seed) # set the random seed of pytorch\n",
    "try : env.seed(seed) # set the random seed of gym env\n",
    "except: pass\n",
    "\n",
    "# # # init/set muzero model for training and inference\n",
    "muzero = Muzero(model_structure = 'mlp_model', # 'vision_model' : will use rgb as observation , 'mlp_model' : will use game state as observation\n",
    "                observation_space_dimensions = env.observation_space, # dimension of the observation \n",
    "                action_space_dimensions = env.action_space, # dimension of the action allow (gym box/discrete)\n",
    "                state_space_dimensions= 31, # support size / encoding space (keep state smaller than hidden layer and use odd number)\n",
    "                hidden_layer_dimensions = 64, # number of weight in the recursive layer of the mlp\n",
    "                number_of_hidden_layer= 4, # number of recusion layer of hidden layer of the mlp\n",
    "                k_hypothetical_steps= 10, # number of future step you want to be simulate during train (they are mainly support loss)\n",
    "                learning_rate= 1e-2,# learning rate of the optimizer\n",
    "                optimizer = \"adam\", # optimizer \"adam\" or \"sgd\"\n",
    "                lr_scheduler = \"cosineannealinglr\",# learning rate scheduler\n",
    "                loss_type = \"general\", # muzero loss can be \"general\" or \"game\"\n",
    "                num_of_epoch = 1000, # number of step use by lr_scheduler\n",
    "                device=\"cpu\", # device on which you want the compute to be made : \"cpu\" , \"cuda\" (it will auto scale on multi gpu or cpu for training and inference)\n",
    "                type_format = torch.float32, # choice the dtype of the model. look at [https://pytorch.org/docs/1.8.1/amp.html#ops-that-can-autocast-to-float16]\n",
    "                load = False, # function to load a save model\n",
    "                use_amp = False, # use mix precision (will get more accuracy than single single precision for smaller dtype like torch.float16. amp do not support torch.float64. will turn amp to True by fault for torch.float16)\n",
    "                bin_method = \"uniform_bin\", # \"linear_bin\" , \"uniform_bin\" : will have a regular incrementation of action or uniform sampling(pick randomly) from the bound\n",
    "                bin_decomposition_number = 10) # number of action to sample from low/high bound of a gym discret box\n",
    "\n",
    "# # # init/set the game storage(stor each game) and dataset(create dataset) generate during training\n",
    "replay_buffer = ReplayBuffer(window_size = 500, # number of game store in the buffer\n",
    "                             batch_size = 128, # batch size is the number of observe game during train\n",
    "                             num_unroll = muzero.k_hypothetical_steps, # number of mouve/play store inside the batched game\n",
    "                             td_steps = 5, # number of step the value is select and scale on \n",
    "                             game_sampling = \"priority\", # 'uniform' or \"priority\" (will game randomly or with a priority distribution)\n",
    "                             position_sampling = \"priority\") # 'uniform' or \"priority\" (will sample position in game randomly or with a priority distribution)\n",
    "\n",
    "# # # init/set the monte carlos tree search parameter\n",
    "mcts = Monte_carlo_tree_search(pb_c_base=19652 , \n",
    "                               pb_c_init=1.25, \n",
    "                               discount= 0.997, \n",
    "                               root_dirichlet_alpha=0.25, \n",
    "                               root_exploration_fraction=0.25)\n",
    "\n",
    "# # # ini/set the Game class which embbed the gym game class function\n",
    "gameplay = Game(gym_env = env, \n",
    "                discount = mcts.discount, #should be the same discount than mcts\n",
    "                limit_of_game_play = 500, # maximum number of mouve , by default float(\"inf\")\n",
    "                observation_dimension = muzero.observation_dimension, \n",
    "                action_dimension = muzero.action_dimension,\n",
    "                rgb_observation = muzero.is_RGB,\n",
    "                action_map = muzero.action_dictionnary)\n",
    "\n",
    "\n",
    "print( f\"Dimension of the observation space : {muzero.observation_dimension} \\\n",
    "         Dimension of the action space : {muzero.action_dimension}\")\n",
    "\n",
    "\n",
    "# # # train model (if you choice vison model it will render the game by opening and closing window)\n",
    "epoch_pr , loss ,reward,learning_config = learning_cycle(number_of_iteration = 1000, # number of epoch(step) in  muzero should be the |total amount of number_of_iteration x number_of_training_before_self_play|\n",
    "                                          number_of_self_play_before_training = 10, # number of game played record in the replay buffer before training\n",
    "                                          number_of_training_before_self_play = 1, # number of epoch cpmpute by the model before selplay\n",
    "                                          number_of_mcts_simulation = 0, # number of mcts simulation ( node expension )\n",
    "                                          model_tag_number = 111, # tag number use to generate checkpoint\n",
    "                                          tempererature_type = \"static_temperature\", # \"static_temperature\" ,\"linear_decrease_temperature\" ,  \"extreme_temperature\" and \"reversal_tanh_temperature\"\n",
    "                                          verbose = True, # if you want to print the epoch|reward|loss during train\n",
    "                                          number_of_worker_selfplay = 0, # \"max\" will set the max amount of cpu core, 0 will make selflay run sequentially. Parallelize self-play on the number of worker\n",
    "                                          muzero_model = muzero,\n",
    "                                          gameplay = gameplay,\n",
    "                                          monte_carlo_tree_search = mcts,\n",
    "                                          replay_buffer = replay_buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_play import report, generate_config_file \n",
    "\n",
    "report( muzero, replay_buffer, epoch_pr, loss, reward, verbose = True)\n",
    "\n",
    "generate_config_file(env,seed,muzero,replay_buffer,mcts,gameplay,learning_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_play import play_game_from_checkpoint \n",
    "import torch\n",
    "\n",
    "play_game_from_checkpoint(game_to_play = 'CartPole-v1',\n",
    "                            \n",
    "                          model_tag = 134,\n",
    "                          model_device = \"cpu\",\n",
    "                          model_type = torch.float32,\n",
    "                            \n",
    "                          mcts_pb_c_base=19652 , \n",
    "                          mcts_pb_c_init=1.25, \n",
    "                          mcts_discount= 0.997, \n",
    "                          mcts_root_dirichlet_alpha=0.25, \n",
    "                          mcts_root_exploration_fraction=0.25,\n",
    "                          mcts_with_or_without_dirichlet_noise = True,\n",
    "                          number_of_monte_carlo_tree_search_simulation = 11,\n",
    "                            \n",
    "                          gameplay_discount = 0.997,\n",
    "                            \n",
    "                          temperature = 0,\n",
    "                          game_iter = 500,\n",
    "                            \n",
    "                          slow_mo_in_second = 0.0,\n",
    "                          render = True,\n",
    "                          verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark \n",
    "from self_play import play_game_from_checkpoint,benchmark\n",
    "import torch\n",
    "\n",
    "number_of_trial = 100\n",
    "cache_t,cache_r,cache_a,cache_p = [],[],[],[]\n",
    "for _ in range(number_of_trial):\n",
    "    tag , reward , action, policy = play_game_from_checkpoint(game_to_play = 'CartPole-v1',\n",
    "                                                                model_tag = 134,\n",
    "                                                                model_device = \"cpu\",\n",
    "                                                                model_type = torch.float32,    \n",
    "                                                                mcts_pb_c_base=19652 , \n",
    "                                                                mcts_pb_c_init=1.25, \n",
    "                                                                mcts_discount= 0.997, \n",
    "                                                                mcts_root_dirichlet_alpha=0.25, \n",
    "                                                                mcts_root_exploration_fraction=0.25,\n",
    "                                                                mcts_with_or_without_dirichlet_noise = True,\n",
    "                                                                number_of_monte_carlo_tree_search_simulation = 11,  \n",
    "                                                                gameplay_discount = 0.997,  \n",
    "                                                                temperature = 0,\n",
    "                                                                game_iter = 500,\n",
    "                                                                slow_mo_in_second = 0,\n",
    "                                                                render = False,\n",
    "                                                                verbose = False,\n",
    "                                                                benchmark = True) # Need benchmark True to return output\n",
    "    #could do it in one list or even wrap the play_game with benchmark but it reduce clarity\n",
    "    cache_t.append(tag)\n",
    "    cache_r.append(reward)\n",
    "    cache_a.append(action)\n",
    "    cache_p.append(policy)\n",
    "\n",
    "\n",
    "benchmark(cache_t,\n",
    "          cache_r,\n",
    "          cache_a,\n",
    "          cache_p,\n",
    "          folder = \"report\",\n",
    "          verbose = True)\n",
    "\n",
    "#on cartpole the reward is fix to 1, so it follow the number of mouve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE WITHOUT COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "from monte_carlo_tree_search import *\n",
    "from game import *\n",
    "from replay_buffer import *\n",
    "from self_play import *\n",
    "from muzero_model import *\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=None) \n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "try : env.seed(seed)\n",
    "except: pass\n",
    "\n",
    "muzero = Muzero(model_structure = 'mlp_model',\n",
    "                observation_space_dimensions = env.observation_space,\n",
    "                action_space_dimensions = env.action_space,\n",
    "                state_space_dimensions= 31, \n",
    "                hidden_layer_dimensions = 64, \n",
    "                number_of_hidden_layer= 0, \n",
    "                k_hypothetical_steps= 10, \n",
    "                learning_rate= 0.01,\n",
    "                optimizer = \"adam\", \n",
    "                lr_scheduler = \"cosineannealinglr\",\n",
    "                loss_type = \"general\", \n",
    "                num_of_epoch = 10000, \n",
    "                device=\"cpu\", \n",
    "                type_format = torch.float32, \n",
    "                load = False, \n",
    "                use_amp = False, \n",
    "                bin_method = \"uniform_bin\", \n",
    "                bin_decomposition_number = 10)\n",
    "\n",
    "replay_buffer = ReplayBuffer(window_size = 500, \n",
    "                             batch_size = 128,\n",
    "                             num_unroll = muzero.k_hypothetical_steps, \n",
    "                             td_steps = 50, \n",
    "                             game_sampling = \"priority\", \n",
    "                             position_sampling = \"priority\") \n",
    "\n",
    "mcts = Monte_carlo_tree_search(pb_c_base=19652 , \n",
    "                               pb_c_init=1.25, \n",
    "                               discount= 0.997, \n",
    "                               root_dirichlet_alpha=0.25, \n",
    "                               root_exploration_fraction=0.25)\n",
    "\n",
    "gameplay = Game(gym_env = env, \n",
    "                discount = mcts.discount,\n",
    "                limit_of_game_play = 500, \n",
    "                observation_dimension = muzero.observation_dimension, \n",
    "                action_dimension = muzero.action_dimension,\n",
    "                rgb_observation = muzero.is_RGB,\n",
    "                action_map = muzero.action_dictionnary)\n",
    "\n",
    "\n",
    "print( f\"Dimension of the observation space : {muzero.observation_dimension} \\\n",
    "         Dimension of the action space : {muzero.action_dimension}\")\n",
    "\n",
    "\n",
    "epoch_pr , loss ,reward,learning_config = learning_cycle(number_of_iteration = 10000, \n",
    "                                          number_of_self_play_before_training = 1, \n",
    "                                          number_of_training_before_self_play = 1, \n",
    "                                          number_of_mcts_simulation = 11,\n",
    "                                          model_tag_number = 123, \n",
    "                                          tempererature_type = \"linear_decrease_temperature\", \n",
    "                                          verbose = True, \n",
    "                                          number_of_worker_selfplay = 0, \n",
    "                                          muzero_model = muzero,\n",
    "                                          gameplay = gameplay,\n",
    "                                          monte_carlo_tree_search = mcts,\n",
    "                                          replay_buffer = replay_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_play import report, generate_config_file \n",
    "\n",
    "report( muzero, replay_buffer, epoch_pr, loss, reward, verbose = True)\n",
    "\n",
    "generate_config_file(env,seed,muzero,replay_buffer,mcts,gameplay,learning_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_play import play_game_from_checkpoint \n",
    "import torch\n",
    "\n",
    "play_game_from_checkpoint(game_to_play = 'CartPole-v1',\n",
    "                            \n",
    "                          model_tag = 123,\n",
    "                          model_device = \"cpu\",\n",
    "                          model_type = torch.float32,\n",
    "                            \n",
    "                          mcts_pb_c_base=19652 , \n",
    "                          mcts_pb_c_init=1.25, \n",
    "                          mcts_discount= 0.997, \n",
    "                          mcts_root_dirichlet_alpha=0.25, \n",
    "                          mcts_root_exploration_fraction=0.25,\n",
    "                          mcts_with_or_without_dirichlet_noise = True,\n",
    "                          number_of_monte_carlo_tree_search_simulation = 11,\n",
    "                            \n",
    "                          gameplay_discount = 0.997,\n",
    "                            \n",
    "                          temperature = 0,\n",
    "                          game_iter = 500,\n",
    "                            \n",
    "                          slow_mo_in_second = 0.0,\n",
    "                          render = True,\n",
    "                          verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark \n",
    "from self_play import play_game_from_checkpoint,benchmark\n",
    "import torch\n",
    "\n",
    "number_of_trial = 100\n",
    "cache_t,cache_r,cache_a,cache_p = [],[],[],[]\n",
    "for _ in range(number_of_trial):\n",
    "    tag , reward , action, policy = play_game_from_checkpoint(game_to_play = 'CartPole-v1',\n",
    "                                                                model_tag = 123,\n",
    "                                                                model_device = \"cpu\",\n",
    "                                                                model_type = torch.float32,    \n",
    "                                                                mcts_pb_c_base=19652 , \n",
    "                                                                mcts_pb_c_init=1.25, \n",
    "                                                                mcts_discount= 0.997, \n",
    "                                                                mcts_root_dirichlet_alpha=0.25, \n",
    "                                                                mcts_root_exploration_fraction=0.25,\n",
    "                                                                mcts_with_or_without_dirichlet_noise = True,\n",
    "                                                                number_of_monte_carlo_tree_search_simulation = 11,  \n",
    "                                                                gameplay_discount = 0.997,  \n",
    "                                                                temperature = 0,\n",
    "                                                                game_iter = 500,\n",
    "                                                                slow_mo_in_second = 0,\n",
    "                                                                render = False,\n",
    "                                                                verbose = False,\n",
    "                                                                benchmark = True) \n",
    "\n",
    "    cache_t.append(tag)\n",
    "    cache_r.append(reward)\n",
    "    cache_a.append(action)\n",
    "    cache_p.append(policy)\n",
    "\n",
    "\n",
    "benchmark(cache_t,\n",
    "          cache_r,\n",
    "          cache_a,\n",
    "          cache_p,\n",
    "          folder = \"report\",\n",
    "          verbose = True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pentos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed8b498c6143e427b0550c5476a07819fd54c33e7113c46248f934dfaabc2ba6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
