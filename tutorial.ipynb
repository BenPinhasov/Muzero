{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # unQuote to install dependency\n",
    "# !pip3 install matplotlib\n",
    "# !pip3 install numpy\n",
    "\n",
    "# # # unQuote to install dependency\n",
    "# # # for Nvidia gpu / cpu version  # I didn't try with ROCm for AMD gpu (pytorch.org) if you want to look at it\n",
    "# update nvidia driver\n",
    "# download and install cuda from : https://developer.nvidia.com/cuda-11-6-1-download-archive\n",
    "# install cudnn :  https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html\n",
    "# !pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "# # # unQuote to install dependency\n",
    "# # # for cpu only | the model is fairly small for cartpole for exampe, so it should be able to run well on cpu only\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "\n",
    "# # # unQuote to install dependency\n",
    "# # # Gym env for game simulation\n",
    "# !pip3 install Box2D\n",
    "# !pip3 install box2d-py\n",
    "# !pip3 install gym[all]\n",
    "# !pip3 install gym[Box_2D]\n",
    "# !pip3 install gym[box2d]\n",
    "\n",
    "##### OR #####\n",
    "# # replace  \" !pip3 \"  by \" !pip \" depending on the OS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from monte_carlo_tree_search import *\n",
    "from game import *\n",
    "from replay_buffer import *\n",
    "from muzero_model import *\n",
    "from self_play import *\n",
    "\n",
    "# # # unquote to print tensor completly\n",
    "# torch.set_printoptions(profile=\"full\")\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "# # # set game environment from gym library\n",
    "env = gym.make('CartPole-v1') # 77min to solve carpole\n",
    "\n",
    "# # # the random seed are set to 0 for reproducibility purpose\n",
    "# # # good reference about it at : https://pytorch.org/docs/stable/notes/randomness.html\n",
    "np.random.seed(0) # set the random seed of numpy\n",
    "torch.manual_seed(0) # set the random seed of pytorch\n",
    "env.seed(0) # set the random seed of gym env\n",
    "\n",
    "# # # init/set muzero model for training and inference\n",
    "muzero = Muzero(model_structure = 'mlp_model', # 'vision_model' : will use rgb as observation , 'mlp_model' : will use game state as observation\n",
    "                observation_space_dimensions = env.observation_space, # dimension of the observation \n",
    "                action_space_dimensions = env.action_space, # dimension of the action allow (gym box/discrete)\n",
    "                state_space_dimensions= 10, # support size / encoding space\n",
    "                hidden_layer_dimensions = 256, # number of weight in the recursive layer of the mlp\n",
    "                number_of_hidden_layer= 5, # number of recusion layer of hidden layer of the mlp\n",
    "                k_hypothetical_steps= 10, # number of future step you want to be simulate during train (they are mainly support loss)\n",
    "                learning_rate= 0.01, # learning rate of the optimizer\n",
    "                num_of_epoch = 10000, # number of step during training (the number of step of self play and training can be change)\n",
    "                device=\"cpu\", # device on which you want the comput to be made : \"cpu\" , \"cuda:0\" , \"cuda:1\" , etc\n",
    "                type_format = torch.float32, # choice the dtype of the model. look at [https://pytorch.org/docs/1.8.1/amp.html#ops-that-can-autocast-to-float16]\n",
    "                load = False, # function for loading a save model\n",
    "                use_amp = False, # use mix precision for gpu (not implement yet)\n",
    "                scaler_on = False, # scale gradient to reduce computation\n",
    "                bin_method = \"uniform_bin\", # \"linear_bin\" , \"uniform_bin\" : will have a regular incrementation of action or uniform sampling(pick randomly) from the bound\n",
    "                bin_decomposition_number = 10) # number of action to sample from low/high bound of a gym discret box\n",
    "\n",
    "# # # init/set the game storage(stor each game) and dataset(create dataset) generate during training\n",
    "replay_buffer = ReplayBuffer(window_size = 500, # number of game store in the buffer\n",
    "                             batch_size = 128, # batch size is the number of observe game during train\n",
    "                             num_unroll = muzero.k_hypothetical_steps, # number of mouve/play store inside the batched game\n",
    "                             td_steps = 50, # number of step the value is scale on \n",
    "                             game_sampling = \"uniform\", # 'uniform' or \"priority\" (will game randomly or with a priority distribution)\n",
    "                             position_sampling = \"uniform\") # 'uniform' or \"priority\" (will sample position in game randomly or with a priority distribution)\n",
    "\n",
    "# # # init/set the monte carlos tree search parameter\n",
    "mcts = Monte_carlo_tree_search(pb_c_base=19652 , \n",
    "                               pb_c_init=1.25, \n",
    "                               discount= 0.95, \n",
    "                               root_dirichlet_alpha=0.25, \n",
    "                               root_exploration_fraction=0.25)\n",
    "\n",
    "# # # ini/set the Game class which embbed the gym game class function\n",
    "gameplay = Game(gym_env = env, \n",
    "                discount = 0.997,\n",
    "                limit_of_game_play = float(\"inf\"), # maximum number of mouve\n",
    "                observation_dimension = muzero.observation_dimension, \n",
    "                action_dimension = muzero.action_dimension,\n",
    "                rgb_observation = muzero.is_RGB,\n",
    "                action_map = muzero.action_dictionnary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # train model (if you choice vison model it will render the game)\n",
    "epoch_pr , loss , reward = learning_cycle(number_of_iteration = 1000, # number of epoch(step) in  muzero should be the |total amount of number_of_iteration x number_of_training_before_self_play|\n",
    "                                          number_of_self_play_before_training = 5, # number of game played record in the replay buffer before training\n",
    "                                          number_of_training_before_self_play = 5, # number of epoch made by the model before selplay\n",
    "                                          number_of_mcts_simulation = 11, \n",
    "                                          model_tag_number = 123, # tag number use to generate checkpoint\n",
    "                                          verbose = True, # if you want to print the epoch|reward|loss during train\n",
    "                                          muzero_model = muzero,\n",
    "                                          gameplay = gameplay,\n",
    "                                          number_of_monte_carlo_tree_search = mcts,\n",
    "                                          replay_buffer = replay_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_play import play_game_from_checkpoint\n",
    "import torch \n",
    "\n",
    "play_game_from_checkpoint(game_to_play = 'CartPole-v1',\n",
    "                            \n",
    "                          model_tag = 124,\n",
    "                          model_device = \"cpu\",\n",
    "                          model_type = torch.float32,\n",
    "                            \n",
    "                          mcts_pb_c_base=19652 , \n",
    "                          mcts_pb_c_init=1.25, \n",
    "                          mcts_discount= 0.95, \n",
    "                          mcts_root_dirichlet_alpha=0.25, \n",
    "                          mcts_root_exploration_fraction=0.25,\n",
    "                          mcts_with_or_without_dirichlet_noise = True,\n",
    "                          number_of_monte_carlo_tree_search_simulation = 11,\n",
    "                            \n",
    "                          gameplay_discount = 0.997,\n",
    "                            \n",
    "                          temperature = 0,\n",
    "                          game_iter = 2000,\n",
    "                            \n",
    "                          slow_mo_in_second = 0.0,\n",
    "                          render = True,\n",
    "                          verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under this cell you will find a manual script to train and play game if you need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # # # Training\n",
    "# number_of_iteration = muzero.epoch\n",
    "# reward , cache_reward , epoch_pr, loss , cache_loss  = [] , [] , [] , [] , []\n",
    "# number_of_self_play_before_training = 1\n",
    "# number_of_training_before_self_play = 1\n",
    "# number_of_mcts_simulation = 11\n",
    "# for ep in range(1,number_of_iteration+1):\n",
    "    \n",
    "#     # # # reset the cache reward for every iteration\n",
    "#     cache_reward = []\n",
    "#     cache_loss = []\n",
    "    \n",
    "#     # # # self_play\n",
    "#     for self_play in range(number_of_self_play_before_training):\n",
    "#         # # # run game with mcts prediction and run step from the policy output \n",
    "#         game = play_game(environment = gameplay, \n",
    "#                          model = muzero,\n",
    "#                          monte_carlo_tree_search = mcts , \n",
    "#                          number_of_monte_carlo_tree_search_simulation = number_of_mcts_simulation,\n",
    "#                          temperature = temperature_scheduler(number_of_iteration+1,ep),\n",
    "#                         ) \n",
    "#         # # # save all the necessary parameter during play_game fortraining\n",
    "#         replay_buffer.save_game(game)\n",
    "#         # # # save the cumulative reward of each self_play\n",
    "#         cache_reward.append(sum(game.rewards))\n",
    "#     # # # sum the average reward of all self_play\n",
    "#     reward.append(sum(cache_reward)/len(cache_reward))\n",
    "    \n",
    "#     # # # save best model. self_play serve as dataset and performace test\n",
    "#     if reward[-1] == max(reward):\n",
    "#         muzero.save_model(directory = \"result\", tag= 124)\n",
    "    \n",
    "#     # # # train model from all game accumulate in the buffer (of the replay_buffer)\n",
    "#     for i in range(number_of_training_before_self_play):\n",
    "#         muzero.train(replay_buffer.sample_batch())\n",
    "#         cache_loss.append(muzero.store_loss[-1][0])\n",
    "#     loss.append(sum(cache_loss)/len(cache_loss))\n",
    "#     prompt_feedback = f'EPOCH {ep} || reward: {reward[-1]} || loss: { loss[-1] }||'\n",
    "#     epoch_pr.append(prompt_feedback)\n",
    "#     print(prompt_feedback)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #play with model of choice\n",
    "\n",
    "# import gym\n",
    "# import time \n",
    "\n",
    "# from monte_carlo_tree_search import *\n",
    "# from game import *\n",
    "# from muzero_model import *\n",
    "# from self_play import *\n",
    "\n",
    "# # # # choice game env\n",
    "# # env = gym.make('LunarLander-v2')\n",
    "# env = gym.make('CartPole-v1')\n",
    "\n",
    "# # # # initialize model class without initializing a neural network\n",
    "# muzero = Muzero(load=True, \n",
    "#                 type_format = torch.float32)\n",
    "\n",
    "# # # # load save model with tag number\n",
    "# muzero.load_model(tag=124,\n",
    "#                   observation_space_dimensions = env.observation_space, \n",
    "#                   device=\"cuda:0\") # set device for model compute\n",
    "\n",
    "# # # # init the mcts class\n",
    "# monte_carlo_tree_search = Monte_carlo_tree_search(pb_c_base=19652 , \n",
    "#                                                     pb_c_init=1.25, \n",
    "#                                                     discount= 0.95, \n",
    "#                                                     root_dirichlet_alpha=0.25, \n",
    "#                                                     root_exploration_fraction=0.25)\n",
    "\n",
    "# # # # create the game class with gameplay/record function\n",
    "# gameplay = Game(env, \n",
    "#                 discount = 0.997,\n",
    "#                 observation_dimension = muzero.observation_dimension, \n",
    "#                 action_dimension = muzero.action_dimension,\n",
    "#                 rgb_observation = muzero.is_RGB,\n",
    "#                 action_map = muzero.action_dictionnary)\n",
    "\n",
    "# # # # slow animation of the render ( in second )\n",
    "# sleep = 0.0\n",
    "\n",
    "# # # # number of simulation for the monte carlos tree search\n",
    "# number_of_monte_carlo_tree_search_simulation = 11\n",
    "\n",
    "# # # # temperature set to 0 will use argmax as policy (highest probability action)\n",
    "# # # # over a temperature of 0.0035 it will sample with the propability associate to the mouve , picking uniformly\n",
    "# temperature = 0\n",
    "\n",
    "# # # # number of iteration (mouve play during the game)\n",
    "# game_iter = 2000\n",
    "\n",
    "# environment = copy.deepcopy(gameplay)\n",
    "# observation_reward_done_info = None\n",
    "\n",
    "# # # # or while not environment.terminal: # for loop to bypass env terminal limit, else use while loop and add a counter variable incrementing\n",
    "# for counter in range(game_iter):\n",
    "    \n",
    "#     #render the env\n",
    "#     environment.vision()\n",
    "    \n",
    "#     # # #laps time to see a slow motion of the env\n",
    "#     time.sleep(sleep)\n",
    "    \n",
    "#     # # # start the game and get game initial observation / game return observation after action\n",
    "#     state = environment.observation(iteration = counter,\n",
    "#                                     feedback = observation_reward_done_info)\n",
    "    \n",
    "#     # # # run monte carlos tree search inference\n",
    "#     # # Train [False or True] mean with or without dirichlet at the root\n",
    "#     mcts = copy.deepcopy(monte_carlo_tree_search)\n",
    "#     policy,tree,action = mcts.run(observation = state, \n",
    "#                                   model = muzero, \n",
    "#                                   num_simulations= number_of_monte_carlo_tree_search_simulation,\n",
    "#                                   train=False)\n",
    "    \n",
    "#     # # # select the best action from policy and inject the action into the game (.step())\n",
    "#     observation_reward_done_info = environment.policy_step(policy = policy, \n",
    "#                                                            action = action,\n",
    "#                                                            temperature =  temperature)\n",
    "    \n",
    "#     # # # reset mcts class to empty cache variable\n",
    "#     mcts.reset()\n",
    "    \n",
    "#     # # # print the number of mouve, action and policy\n",
    "#     print(f\"Mouve number: {counter+1} , Action: {muzero.action_dictionnary[action[np.argmax(policy/policy.sum())]]}, Policy: {policy/policy.sum()}\")\n",
    "# environment.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('tradd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9576c99980f95e051990d4e759e2b8bed23c4dbd32ead604c26a9cabcb988379"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
